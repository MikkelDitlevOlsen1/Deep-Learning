{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba09f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "import torch \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc711627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#change current directory up to parent, only run 1 time!\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a791fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = [0,1,2,3,5,6,10,11,12,19,20,21,22,24,26,28,29,32,33,35,36,39,40,43,45,46,47,50,51,52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = ['0_a', '1_a', '2_a', '3_a', '5', '5_a', '6', '6_a', '7', '8', '9', '10_a', '11_a', '12', '12_a', '13', '15', '16', '19_a', '20_a', '21_a', '22_a', '24_a', '26', '26_a', '28_a', '29', '29_a', '31', '32', '32_a', '33', '33_a', '34', '35_a', '36_a', '37', '38', '39', '39_a', '40_a', '41', '42', '43_a', '44', '45', '45_a', '46_a', '47_a', '50_a', '51_a', '52_a', '54', '54_a', '57', '57_a', '59', '60_a', '62', '62_a', '64', '64_a', '65_a', '66', '66_a', '68_a', '69', '71_a', '72_a', '73', '73_a', '74', '74_a', '75_a', '78_a', '79_a', '82', '83_a', '84', '85', '86_a', '87_a', '93', '94_a', '95_a', '99_a', '102_a', '103', '104_a', '108', '112_a', '113_a', '114', '114_a', '116', '120', '120_a', '121', '121_a', '124_a', '125_a', '130', '137_a', '141_a', '147', '153_a', '156_a', '158', '158_a', '180', '182', '192', '193', '199_a', '201', '209', '211', '224', '224_a', '237', '241', '249_a', '259', '272', '281_a', '283_a', '285_a', '290', '301', '308', '312_a', '313', '331', '334_a', '341_a', '351_a', '353_a', '361', '363_a', '379_a', '381_a', '399_a', '409_a', '412', '417_a', '428_a', '455', '471', '475', '493', '499', '518', '519_a', '535_a', '536', '557_a', '582_a', '606_a', '664_a', '699_a', '700_a', '706_a', '724_a', '773_a', '795_a', '798_a', '801_a', '816_a', '855_a']\n",
    "#loads data from clean_data \n",
    "#path need to be set to the clean_data folder\n",
    "def load_data(test=False,Print=False,path=f'{os.path.abspath(os.curdir)}/Deep_Learning_Project/Deep-Learning-main/data',nr_img=1499):\n",
    "    train_data_input=[]\n",
    "    train_data_target=[]\n",
    "    #path1 = f'{os.path.abspath(os.curdir)}/data/carseg_data/carseg_raw_data/train/photo'\n",
    "    \n",
    "    if test:\n",
    "        for i in range (len(test_id)):\n",
    "            n = test_id[i]\n",
    "            n1 = f'{n}_a'\n",
    "            test1=np.load(f'{path}/{n1}.npy')\n",
    "            inputs=torch.from_numpy(test1[:3])\n",
    "            target=torch.from_numpy(test1[3])\n",
    "            #target= torch.unsqueeze(target,0)\n",
    "            train_data_input.append(inputs)\n",
    "            train_data_target.append(target)\n",
    "            \n",
    "            \n",
    "            \n",
    "    else:\n",
    " \n",
    "        for n in range(nr_img):\n",
    "            n1=n\n",
    "            \n",
    "            if (n1 not in test_id):\n",
    "                n2 = f'{n}_a'\n",
    "                \n",
    "                if (f'{n2}' in photo):\n",
    "                    test2=np.load(f'{path}/{n2}.npy')\n",
    "                    inputs2=torch.from_numpy(test2[:3])\n",
    "                    target2=torch.from_numpy(test2[3])\n",
    "                    #target= torch.unsqueeze(target,0)\n",
    "                    train_data_input.append(inputs2)\n",
    "                    train_data_target.append(target2)\n",
    "            \n",
    "            #if test:\n",
    "            #    if (n1 in test_id):\n",
    "            #        #n1=f'{n}_a'\n",
    "            \n",
    "            if (f'{n1}' in photo):\n",
    "\t\t\t\t#os.path.exists(f'{path1}/{n1}.jpg'):\n",
    "                test1=np.load(f'{path}/{n1}.npy')\n",
    "                inputs=torch.from_numpy(test1[:3])\n",
    "                target=torch.from_numpy(test1[3])\n",
    "                #target= torch.unsqueeze(target,0)\n",
    "                train_data_input.append(inputs)\n",
    "                train_data_target.append(target)\n",
    "                \n",
    "            \n",
    "           \n",
    "    return [train_data_input,train_data_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b88799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load test and train data\n",
    "test_data=load_data(test=True)\n",
    "train_data=load_data()\n",
    "\n",
    "#split op into x and y\n",
    "test_x, test_y = test_data[0], test_data[1]\n",
    "train_x, train_y = train_data[0], train_data[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the form of data is:\n",
    "#data[0] is a list with all rgb img's as tensor\n",
    "#data[1] is a list with all targets as tensor  \n",
    "def Get_stats(data):\n",
    "    print(f'Number of img {len(data[0])}')\n",
    "    print(f'target sahpe {data[1][0].shape}')\n",
    "    print(f'input shape {data[0][0].shape}')\n",
    "    \n",
    "    \n",
    "#print some info about data structure\n",
    "print('test_data:')\n",
    "Get_stats(test_data)\n",
    "print('train_data:')\n",
    "Get_stats(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range (len(train_x)):\n",
    "#    a = torch.permute(train_x[i], (1,2,0))\n",
    "#    plt.imshow(a)\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "from torchvision import transforms\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create data structure\n",
    "from torch.utils import data\n",
    "class batches(data.Dataset):\n",
    "    def __init__(self, x_set, y_set,transform=None):\n",
    "        self.transform=transform\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.long\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx]\n",
    "        batch_y = self.y[idx]\n",
    "        if self.transform:\n",
    "            batch_x = self.transform(batch_x)\n",
    "        return batch_x.type(self.inputs_dtype),batch_y.type(self.inputs_dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # one hot encoder\n",
    "# \n",
    "\n",
    "def one_hot_target(target):\n",
    "    l=[0]*len(target)\n",
    "    for targets in range(len(target)):\n",
    "        l[targets] = one_hot(target[targets].to(torch.int64), num_classes=9)\n",
    "        l[targets] = torch.permute(l[targets], (2,0,1))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_y_one=one_hot_target(test_y)\n",
    "train_y_one=one_hot_target(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f368d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('test_data:')\n",
    "Get_stats([test_x, test_y_one])\n",
    "print('train_data:')\n",
    "Get_stats([train_x, train_y_one])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31604d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# based on:\n",
    "# https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    r\"\"\"Criterion that computes Sørensen-Dice Coefficient loss.\n",
    "\n",
    "    According to [1], we compute the Sørensen-Dice Coefficient as follows:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\text{Dice}(x, class) = \\frac{2 |X| \\cap |Y|}{|X| + |Y|}\n",
    "\n",
    "    where:\n",
    "       - :math:`X` expects to be the scores of each class.\n",
    "       - :math:`Y` expects to be the one-hot tensor with the class labels.\n",
    "\n",
    "    the loss, is finally computed as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\text{loss}(x, class) = 1 - \\text{Dice}(x, class)\n",
    "\n",
    "    [1] https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)` where C = number of classes.\n",
    "        - Target: :math:`(N, H, W)` where each value is\n",
    "          :math:`0 ≤ targets[i] ≤ C−1`.\n",
    "\n",
    "    Examples:\n",
    "        >>> N = 5  # num_classes\n",
    "        >>> loss = tgm.losses.DiceLoss()\n",
    "        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n",
    "        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n",
    "        >>> output = loss(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.eps: float = 1e-6\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input: torch.Tensor,\n",
    "            target: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                             .format(input.shape))\n",
    "        if not input.shape[-2:] == target.shape[-2:]:\n",
    "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                             .format(input.shape, input.shape))\n",
    "        if not input.device == target.device:\n",
    "            raise ValueError(\n",
    "                \"input and target must be in the same device. Got: {}\" .format(\n",
    "                    input.device, target.device))\n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = F.softmax(input, dim=1)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        #target_one_hot = one_hot(target, num_classes=input.shape[1],\n",
    "        #                         device=input.device, dtype=input.dtype)\n",
    "\n",
    "        # compute the actual dice score\n",
    "        dims = (1, 2, 3)\n",
    "        intersection = torch.sum(input_soft * target, dims)\n",
    "        cardinality = torch.sum(input_soft + target, dims)\n",
    "\n",
    "        dice_score = 2. * intersection / (cardinality + self.eps)\n",
    "        return torch.mean(1. - dice_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dice_loss(\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Function that computes Sørensen-Dice Coefficient loss.\n",
    "\n",
    "    See :class:`~torchgeometry.losses.DiceLoss` for details.\n",
    "    \"\"\"\n",
    "    return DiceLoss()(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_x=train_x[:20]\n",
    "ttrain_x=train_x[20:]\n",
    "\n",
    "val_y_one=train_y_one[:20]\n",
    "ttrain_y_one=train_y_one[20:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creats all the layers we need\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Center-crops the encoder_layer to the size of the decoder_layer,\n",
    "    so that merging (concatenation) between levels/blocks is possible.\n",
    "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n",
    "    \"\"\"\n",
    "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n",
    "        ds = encoder_layer.shape[2:]\n",
    "        es = decoder_layer.shape[2:]\n",
    "        assert ds[0] >= es[0]\n",
    "        assert ds[1] >= es[1]\n",
    "        if encoder_layer.dim() == 4:  # 2D\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\n",
    "                            ]\n",
    "        elif encoder_layer.dim() == 5:  # 3D\n",
    "            assert ds[2] >= es[2]\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\n",
    "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\n",
    "                            ]\n",
    "    return encoder_layer, decoder_layer\n",
    "\n",
    "\n",
    "def conv_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.Conv3d\n",
    "    elif dim == 2:\n",
    "        return nn.Conv2d\n",
    "\n",
    "\n",
    "def get_conv_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 3,\n",
    "                   stride: int = 1,\n",
    "                   padding: int = 1,\n",
    "                   bias: bool = True,\n",
    "                   dim: int = 2):\n",
    "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                           bias=bias)\n",
    "\n",
    "\n",
    "def conv_transpose_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.ConvTranspose3d\n",
    "    elif dim == 2:\n",
    "        return nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def get_up_layer(in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 2,\n",
    "                 dim: int = 3,\n",
    "                 up_mode: str = 'transposed',\n",
    "                 ):\n",
    "    if up_mode == 'transposed':\n",
    "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n",
    "\n",
    "\n",
    "def maxpool_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.MaxPool3d\n",
    "    elif dim == 2:\n",
    "        return nn.MaxPool2d\n",
    "\n",
    "\n",
    "def get_maxpool_layer(kernel_size: int = 2,\n",
    "                      stride: int = 2,\n",
    "                      padding: int = 0,\n",
    "                      dim: int = 2):\n",
    "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "def get_activation(activation: str):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'leaky':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU()\n",
    "\n",
    "\n",
    "def get_normalization(normalization: str,\n",
    "                      num_channels: int,\n",
    "                      dim: int):\n",
    "    if normalization == 'batch':\n",
    "        if dim == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "    elif normalization == 'instance':\n",
    "        if dim == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "    elif 'group' in normalization:\n",
    "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, layer_1, layer_2):\n",
    "        x = torch.cat((layer_1, layer_2), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 MaxPool.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 pooling: bool = True,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: str = 2,\n",
    "                 conv_mode: str = 'same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # pooling layer\n",
    "        if self.pooling:\n",
    "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # activation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "\n",
    "        before_pooling = y  # save the outputs before the pooling operation\n",
    "        if self.pooling:\n",
    "            y = self.pool(y)  # pooling\n",
    "        return y, before_pooling\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: int = 3,\n",
    "                 conv_mode: str = 'same',\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        # upconvolution/upsample layer\n",
    "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "        # conv layers\n",
    "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\n",
    "                                    padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act0 = get_activation(self.activation)\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "        # concatenate layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def forward(self, encoder_layer, decoder_layer):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            encoder_layer: Tensor from the encoder pathway\n",
    "            decoder_layer: Tensor from the decoder pathway (to be up'd)\n",
    "        \"\"\"\n",
    "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n",
    "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n",
    "\n",
    "        if self.up_mode != 'transposed':\n",
    "            # We need to reduce the channel dimension with a conv layer\n",
    "            up_layer = self.conv0(up_layer)  # convolution 0\n",
    "        up_layer = self.act0(up_layer)  # activation 0\n",
    "        if self.normalization:\n",
    "            up_layer = self.norm0(up_layer)  # normalization 0\n",
    "\n",
    "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n",
    "        y = self.conv1(merged_layer)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # acivation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "        return y\n",
    "\n",
    "# creat the model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 out_channels: int = 8,\n",
    "                 n_blocks: int = 4,\n",
    "                 start_filters: int = 32,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = 'batch',\n",
    "                 conv_mode: str = 'same',\n",
    "                 dim: int = 2,\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_blocks = n_blocks\n",
    "        self.start_filters = start_filters\n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.conv_mode = conv_mode\n",
    "        self.dim = dim\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "\n",
    "        # create encoder path\n",
    "        for i in range(self.n_blocks):\n",
    "            num_filters_in = self.in_channels if i == 0 else num_filters_out\n",
    "            num_filters_out = self.start_filters * (2 ** i)\n",
    "            pooling = True if i < self.n_blocks - 1 else False\n",
    "\n",
    "            down_block = DownBlock(in_channels=num_filters_in,\n",
    "                                   out_channels=num_filters_out,\n",
    "                                   pooling=pooling,\n",
    "                                   activation=self.activation,\n",
    "                                   normalization=self.normalization,\n",
    "                                   conv_mode=self.conv_mode,\n",
    "                                   dim=self.dim)\n",
    "\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # create decoder path (requires only n_blocks-1 blocks)\n",
    "        for i in range(n_blocks - 1):\n",
    "            num_filters_in = num_filters_out\n",
    "            num_filters_out = num_filters_in // 2\n",
    "\n",
    "            up_block = UpBlock(in_channels=num_filters_in,\n",
    "                               out_channels=num_filters_out,\n",
    "                               activation=self.activation,\n",
    "                               normalization=self.normalization,\n",
    "                               conv_mode=self.conv_mode,\n",
    "                               dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # final convolution\n",
    "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                         bias=True, dim=self.dim)\n",
    "        \n",
    "        # add the list of modules to current module\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.weight, **kwargs)  # weights\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.bias, **kwargs)  # bias\n",
    "\n",
    "    def initialize_parameters(self,\n",
    "                              method_weights=nn.init.xavier_uniform_,\n",
    "                              method_bias=nn.init.zeros_,\n",
    "                              kwargs_weights={},\n",
    "                              kwargs_bias={}\n",
    "                              ):\n",
    "        for module in self.modules():\n",
    "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\n",
    "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        encoder_output = []\n",
    "\n",
    "        # Encoder pathway\n",
    "        for module in self.down_blocks:\n",
    "            x, before_pooling = module(x)\n",
    "            encoder_output.append(before_pooling)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for i, module in enumerate(self.up_blocks):\n",
    "            before_pool = encoder_output[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "        x = torch.softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\n",
    "        d = {self.__class__.__name__: attributes}\n",
    "        return f'{d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creat a model with 4 bloks\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=9,\n",
    "             n_blocks=4,\n",
    "             start_filters=9,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "\n",
    "x = torch.randn(size=(4,3, 256, 256), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(f'Out: {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary = summary(model, (3, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cheak number of \"bloks\" and its resalutien on that blok\n",
    "shape = 256\n",
    "def compute_max_depth(shape, max_depth=10, print_out=True):\n",
    "    shapes = []\n",
    "    shapes.append(shape)\n",
    "    for level in range(1, max_depth):\n",
    "        if shape % 2 ** level == 0 and shape / 2 ** level > 1:\n",
    "            shapes.append(shape / 2 ** level)\n",
    "            if print_out:\n",
    "                print(f'Level {level}: {shape / 2 ** level}')\n",
    "        else:\n",
    "            if print_out:\n",
    "                print(f'Max-level: {level - 1}')\n",
    "            break\n",
    "\n",
    "    return shapes\n",
    "\n",
    "\n",
    "out = compute_max_depth(shape, print_out=True, max_depth=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c088b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train data\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "\n",
    "        batch_iter.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd962249",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transform=None\n",
    "traindata=batches(ttrain_x,ttrain_y_one,transform=data_transform)\n",
    "\n",
    "valdata=batches(val_x,val_y_one)\n",
    "\n",
    "testdata=batches(test_x,test_y_one)\n",
    "\n",
    "\n",
    "#creat data batches\n",
    "dataloader_training = data.DataLoader(dataset=traindata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "dataloader_val = data.DataLoader(dataset=valdata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "dataloader_train= data.DataLoader(dataset=testdata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train the mo\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "# model\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=9,\n",
    "             n_blocks=3,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2).to(device)\n",
    "\n",
    "# criterion\n",
    "#\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = DiceLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=dataloader_training,\n",
    "                  validation_DataLoader=dataloader_val,\n",
    "                  lr_scheduler=None,\n",
    "                  epochs=100,\n",
    "                  epoch=0,\n",
    "                  notebook=False)\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82151d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(training_losses)\n",
    "print(validation_losses)\n",
    "\n",
    "\n",
    "with open('validation_t12.pickle','wb') as f:\n",
    "\tpickle.dump(validation_losses, f)\n",
    "\n",
    "with open('training_t12.pickle','wb') as f:\n",
    "\tpickle.dump(training_losses, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
