{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "import torch \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change current directory up to parent, only run 1 time!\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads data from clean_data \n",
    "#path need to be set to the clean_data folder\n",
    "def load_data(test=False,Print=False,path=f'{os.path.abspath(os.curdir)}/data/carseg_data/clean_data',nr_img=1498):\n",
    "    train_data_input=[]\n",
    "    train_data_target=[]\n",
    "    \n",
    "    for n in range(nr_img):\n",
    "        n1=n\n",
    "        if test:\n",
    "            n1=f'{n}_a'\n",
    "        try:\n",
    "            test1=np.load(f'{path}/{n1}.npy')\n",
    "            inputs=torch.from_numpy(test1[:3])\n",
    "            target=torch.from_numpy(test1[3])\n",
    "            #target= torch.unsqueeze(target,0)\n",
    "            train_data_input.append(inputs)\n",
    "            train_data_target.append(target)\n",
    "        except:\n",
    "            if Print:\n",
    "                print(f'fil nr {n} mangeler')\n",
    "    return [train_data_input,train_data_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test and train data\n",
    "test_data=load_data(test=True)\n",
    "train_data=load_data()\n",
    "\n",
    "#split op into x and y\n",
    "test_x, test_y = test_data[0], test_data[1]\n",
    "train_x, train_y = train_data[0], train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:\n",
      "Number of img 99\n",
      "target sahpe torch.Size([1, 256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n",
      "train_data:\n",
      "Number of img 1128\n",
      "target sahpe torch.Size([1, 256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#the form of data is:\n",
    "#data[0] is a list with all rgb img's as tensor\n",
    "#data[1] is a list with all targets as tensor  \n",
    "def Get_stats(data):\n",
    "    print(f'Number of img {len(data[0])}')\n",
    "    print(f'target sahpe {data[1][0].shape}')\n",
    "    print(f'input shape {data[0][0].shape}')\n",
    "    \n",
    "    \n",
    "#print some info about data structure\n",
    "print('test_data:')\n",
    "Get_stats(test_data)\n",
    "print('train_data:')\n",
    "Get_stats(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_one_hot = one_hot(target.to(torch.int64), num_classes=9)\n",
    "target_n = torch.argmax(target_one_hot, dim = -1)\n",
    "\n",
    "plt.imshow(target_n)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_target (target):\n",
    "    for targets in range (len(target)):\n",
    "        target[targets] = one_hot(target[targets].to(torch.int64), num_classes=9)\n",
    "        \n",
    "    return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 256, 256, 9, 9) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2502fcc274a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2724\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    707\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 709\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    710\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 256, 256, 9, 9) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMX0lEQVR4nO3bX4il9X3H8fenuxEak0aJk5DuKt2WNbotWnRiJPSPaWizay6WgBdqqFQCixBDLpVCk4I3zUUhBP8siyySm+xNJN0UEyktiQVr4yz4bxVlulKdrOAaQwoGKqvfXsxpc3q+szvPrGfO2cH3CwbmeZ7fOefLMOc9zzzzTKoKSRr3G/MeQNL5xzBIagyDpMYwSGoMg6TGMEhq1g1DksNJXk/y3BmOJ8m3kywneSbJNdMfU9IsDTljeAjYe5bj+4Ddo48DwAPvfSxJ87RuGKrqMeDNsyzZD3ynVj0BXJTkE9MaUNLsbZ/Cc+wAXh3bXhnte21yYZIDrJ5VcOGFF157xRVXTOHlJZ3JsWPH3qiqhY0+bhphyBr71rzPuqoOAYcAFhcXa2lpaQovL+lMkvznuTxuGn+VWAEuHdveCZycwvNKmpNphOEocNvorxPXA7+sqvZrhKStY91fJZJ8F7gBuCTJCvAN4AMAVXUQeAS4EVgGfgXcvlnDSpqNdcNQVbesc7yAr0xtIklz552PkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySmkFhSLI3yYtJlpPcvcbxjyT5QZKnkxxPcvv0R5U0K+uGIck24D5gH7AHuCXJnollXwGer6qrgRuAv09ywZRnlTQjQ84YrgOWq+pEVb0NHAH2T6wp4MNJAnwIeBM4PdVJJc3MkDDsAF4d214Z7Rt3L3AlcBJ4FvhaVb07+URJDiRZSrJ06tSpcxxZ0mYbEoassa8mtj8PPAX8NvCHwL1Jfqs9qOpQVS1W1eLCwsIGR5U0K0PCsAJcOra9k9Uzg3G3Aw/XqmXgZeCK6YwoadaGhOFJYHeSXaMLijcDRyfWvAJ8DiDJx4FPAiemOaik2dm+3oKqOp3kTuBRYBtwuKqOJ7ljdPwgcA/wUJJnWf3V466qemMT55a0idYNA0BVPQI8MrHv4NjnJ4G/mO5okubFOx8lNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVIzKAxJ9iZ5MclykrvPsOaGJE8lOZ7kJ9MdU9IsbV9vQZJtwH3AnwMrwJNJjlbV82NrLgLuB/ZW1StJPrZJ80qagSFnDNcBy1V1oqreBo4A+yfW3Ao8XFWvAFTV69MdU9IsDQnDDuDVse2V0b5xlwMXJ/lxkmNJblvriZIcSLKUZOnUqVPnNrGkTTckDFljX01sbweuBb4AfB74mySXtwdVHaqqxapaXFhY2PCwkmZj3WsMrJ4hXDq2vRM4ucaaN6rqLeCtJI8BVwMvTWVKSTM15IzhSWB3kl1JLgBuBo5OrPkH4I+TbE/yQeDTwAvTHVXSrKx7xlBVp5PcCTwKbAMOV9XxJHeMjh+sqheS/Ah4BngXeLCqntvMwSVtnlRNXi6YjcXFxVpaWprLa0vvF0mOVdXiRh/nnY+SGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJKaQWFIsjfJi0mWk9x9lnWfSvJOkpumN6KkWVs3DEm2AfcB+4A9wC1J9pxh3TeBR6c9pKTZGnLGcB2wXFUnqupt4Aiwf411XwW+B7w+xfkkzcGQMOwAXh3bXhnt+z9JdgBfBA6e7YmSHEiylGTp1KlTG51V0owMCUPW2FcT298C7qqqd872RFV1qKoWq2pxYWFh4IiSZm37gDUrwKVj2zuBkxNrFoEjSQAuAW5Mcrqqvj+NISXN1pAwPAnsTrIL+BlwM3Dr+IKq2vW/nyd5CPhHoyBtXeuGoapOJ7mT1b82bAMOV9XxJHeMjp/1uoKkrWfIGQNV9QjwyMS+NYNQVX/13seSNE/e+SipMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkppBYUiyN8mLSZaT3L3G8S8leWb08XiSq6c/qqRZWTcMSbYB9wH7gD3ALUn2TCx7GfjTqroKuAc4NO1BJc3OkDOG64DlqjpRVW8DR4D94wuq6vGq+sVo8wlg53THlDRLQ8KwA3h1bHtltO9Mvgz8cK0DSQ4kWUqydOrUqeFTSpqpIWHIGvtqzYXJZ1kNw11rHa+qQ1W1WFWLCwsLw6eUNFPbB6xZAS4d294JnJxclOQq4EFgX1X9fDrjSZqHIWcMTwK7k+xKcgFwM3B0fEGSy4CHgb+sqpemP6akWVr3jKGqTie5E3gU2AYcrqrjSe4YHT8IfB34KHB/EoDTVbW4eWNL2kypWvNywaZbXFyspaWluby29H6R5Ni5/JD2zkdJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBknNoDAk2ZvkxSTLSe5e43iSfHt0/Jkk10x/VEmzsm4YkmwD7gP2AXuAW5LsmVi2D9g9+jgAPDDlOSXN0JAzhuuA5ao6UVVvA0eA/RNr9gPfqVVPABcl+cSUZ5U0I9sHrNkBvDq2vQJ8esCaHcBr44uSHGD1jALgv5M8t6Fp5+sS4I15DzHQVpoVtta8W2lWgE+ey4OGhCFr7KtzWENVHQIOASRZqqrFAa9/XthK826lWWFrzbuVZoXVec/lcUN+lVgBLh3b3gmcPIc1kraIIWF4EtidZFeSC4CbgaMTa44Ct43+OnE98Muqem3yiSRtDev+KlFVp5PcCTwKbAMOV9XxJHeMjh8EHgFuBJaBXwG3D3jtQ+c89XxspXm30qywtebdSrPCOc6bqnYpQNL7nHc+SmoMg6Rm08OwlW6nHjDrl0YzPpPk8SRXz2POsXnOOu/Yuk8leSfJTbOcb2KGdWdNckOSp5IcT/KTWc84Mct63wsfSfKDJE+P5h1yXW1TJDmc5PUz3Rd0Tu+xqtq0D1YvVv4H8LvABcDTwJ6JNTcCP2T1XojrgX/fzJne46yfAS4efb5vXrMOnXds3b+weoH4pvN1VuAi4HngstH2x87nry3w18A3R58vAG8CF8xp3j8BrgGeO8PxDb/HNvuMYSvdTr3urFX1eFX9YrT5BKv3a8zLkK8twFeB7wGvz3K4CUNmvRV4uKpeAaiq833eAj6cJMCHWA3D6dmOORqk6rHR65/Jht9jmx2GM90qvdE1s7DROb7MaoXnZd15k+wAvggcnOFcaxnytb0cuDjJj5McS3LbzKbrhsx7L3AlqzfyPQt8rarenc14G7bh99iQW6Lfi6ndTj0Dg+dI8llWw/BHmzrR2Q2Z91vAXVX1zuoPtrkZMut24Frgc8BvAv+W5Imqemmzh1vDkHk/DzwF/Bnwe8A/JfnXqvqvTZ7tXGz4PbbZYdhKt1MPmiPJVcCDwL6q+vmMZlvLkHkXgSOjKFwC3JjkdFV9fyYT/trQ74M3quot4K0kjwFXA/MIw5B5bwf+rlZ/iV9O8jJwBfDT2Yy4IRt/j23yRZHtwAlgF7++iPP7E2u+wP+/MPLTOV3AGTLrZaze3fmZecy40Xkn1j/E/C4+DvnaXgn882jtB4HngD84j+d9APjb0ecfB34GXDLH74ff4cwXHzf8HtvUM4bavNup5zXr14GPAvePfgqfrjn9p93Aec8LQ2atqheS/Ah4BngXeLCq5vJv+QO/tvcADyV5ltU33F1VNZd/x07yXeAG4JIkK8A3gA+Mzbrh95i3REtqvPNRUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUvM/YA1djYGMYyEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = one_hot_target(test_y)\n",
    "\n",
    "plt.imshow(torch.argmax(a[0], dim = -1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Center-crops the encoder_layer to the size of the decoder_layer,\n",
    "    so that merging (concatenation) between levels/blocks is possible.\n",
    "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n",
    "    \"\"\"\n",
    "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n",
    "        ds = encoder_layer.shape[2:]\n",
    "        es = decoder_layer.shape[2:]\n",
    "        assert ds[0] >= es[0]\n",
    "        assert ds[1] >= es[1]\n",
    "        if encoder_layer.dim() == 4:  # 2D\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\n",
    "                            ]\n",
    "        elif encoder_layer.dim() == 5:  # 3D\n",
    "            assert ds[2] >= es[2]\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\n",
    "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\n",
    "                            ]\n",
    "    return encoder_layer, decoder_layer\n",
    "\n",
    "\n",
    "def conv_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.Conv3d\n",
    "    elif dim == 2:\n",
    "        return nn.Conv2d\n",
    "\n",
    "\n",
    "def get_conv_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 3,\n",
    "                   stride: int = 1,\n",
    "                   padding: int = 1,\n",
    "                   bias: bool = True,\n",
    "                   dim: int = 2):\n",
    "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                           bias=bias)\n",
    "\n",
    "\n",
    "def conv_transpose_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.ConvTranspose3d\n",
    "    elif dim == 2:\n",
    "        return nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def get_up_layer(in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 2,\n",
    "                 dim: int = 3,\n",
    "                 up_mode: str = 'transposed',\n",
    "                 ):\n",
    "    if up_mode == 'transposed':\n",
    "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n",
    "\n",
    "\n",
    "def maxpool_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.MaxPool3d\n",
    "    elif dim == 2:\n",
    "        return nn.MaxPool2d\n",
    "\n",
    "\n",
    "def get_maxpool_layer(kernel_size: int = 2,\n",
    "                      stride: int = 2,\n",
    "                      padding: int = 0,\n",
    "                      dim: int = 2):\n",
    "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "def get_activation(activation: str):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'leaky':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU()\n",
    "\n",
    "\n",
    "def get_normalization(normalization: str,\n",
    "                      num_channels: int,\n",
    "                      dim: int):\n",
    "    if normalization == 'batch':\n",
    "        if dim == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "    elif normalization == 'instance':\n",
    "        if dim == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "    elif 'group' in normalization:\n",
    "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, layer_1, layer_2):\n",
    "        x = torch.cat((layer_1, layer_2), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 MaxPool.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 pooling: bool = True,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: str = 2,\n",
    "                 conv_mode: str = 'same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # pooling layer\n",
    "        if self.pooling:\n",
    "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # activation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "\n",
    "        before_pooling = y  # save the outputs before the pooling operation\n",
    "        if self.pooling:\n",
    "            y = self.pool(y)  # pooling\n",
    "        return y, before_pooling\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: int = 3,\n",
    "                 conv_mode: str = 'same',\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        # upconvolution/upsample layer\n",
    "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "        # conv layers\n",
    "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\n",
    "                                    padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act0 = get_activation(self.activation)\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "        # concatenate layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def forward(self, encoder_layer, decoder_layer):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            encoder_layer: Tensor from the encoder pathway\n",
    "            decoder_layer: Tensor from the decoder pathway (to be up'd)\n",
    "        \"\"\"\n",
    "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n",
    "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n",
    "\n",
    "        if self.up_mode != 'transposed':\n",
    "            # We need to reduce the channel dimension with a conv layer\n",
    "            up_layer = self.conv0(up_layer)  # convolution 0\n",
    "        up_layer = self.act0(up_layer)  # activation 0\n",
    "        if self.normalization:\n",
    "            up_layer = self.norm0(up_layer)  # normalization 0\n",
    "\n",
    "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n",
    "        y = self.conv1(merged_layer)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # acivation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "        return y\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 out_channels: int = 8,\n",
    "                 n_blocks: int = 4,\n",
    "                 start_filters: int = 32,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = 'batch',\n",
    "                 conv_mode: str = 'same',\n",
    "                 dim: int = 2,\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_blocks = n_blocks\n",
    "        self.start_filters = start_filters\n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.conv_mode = conv_mode\n",
    "        self.dim = dim\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "\n",
    "        # create encoder path\n",
    "        for i in range(self.n_blocks):\n",
    "            num_filters_in = self.in_channels if i == 0 else num_filters_out\n",
    "            num_filters_out = self.start_filters * (2 ** i)\n",
    "            pooling = True if i < self.n_blocks - 1 else False\n",
    "\n",
    "            down_block = DownBlock(in_channels=num_filters_in,\n",
    "                                   out_channels=num_filters_out,\n",
    "                                   pooling=pooling,\n",
    "                                   activation=self.activation,\n",
    "                                   normalization=self.normalization,\n",
    "                                   conv_mode=self.conv_mode,\n",
    "                                   dim=self.dim)\n",
    "\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # create decoder path (requires only n_blocks-1 blocks)\n",
    "        for i in range(n_blocks - 1):\n",
    "            num_filters_in = num_filters_out\n",
    "            num_filters_out = num_filters_in // 2\n",
    "\n",
    "            up_block = UpBlock(in_channels=num_filters_in,\n",
    "                               out_channels=num_filters_out,\n",
    "                               activation=self.activation,\n",
    "                               normalization=self.normalization,\n",
    "                               conv_mode=self.conv_mode,\n",
    "                               dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # final convolution\n",
    "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                         bias=True, dim=self.dim)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
    "\n",
    "        # initialize the weights\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.weight, **kwargs)  # weights\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.bias, **kwargs)  # bias\n",
    "\n",
    "    def initialize_parameters(self,\n",
    "                              method_weights=nn.init.xavier_uniform_,\n",
    "                              method_bias=nn.init.zeros_,\n",
    "                              kwargs_weights={},\n",
    "                              kwargs_bias={}\n",
    "                              ):\n",
    "        for module in self.modules():\n",
    "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\n",
    "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        encoder_output = []\n",
    "\n",
    "        # Encoder pathway\n",
    "        for module in self.down_blocks:\n",
    "            x, before_pooling = module(x)\n",
    "            encoder_output.append(before_pooling)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for i, module in enumerate(self.up_blocks):\n",
    "            before_pool = encoder_output[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\n",
    "        d = {self.__class__.__name__: attributes}\n",
    "        return f'{d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out: torch.Size([4, 9, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=3,\n",
    "             out_channels=9,\n",
    "             n_blocks=4,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "\n",
    "x = torch.randn(size=(4,3, 256, 256), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(f'Out: {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 256]             896\n",
      "              ReLU-2         [-1, 32, 256, 256]               0\n",
      "       BatchNorm2d-3         [-1, 32, 256, 256]              64\n",
      "            Conv2d-4         [-1, 32, 256, 256]           9,248\n",
      "              ReLU-5         [-1, 32, 256, 256]               0\n",
      "       BatchNorm2d-6         [-1, 32, 256, 256]              64\n",
      "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
      "         DownBlock-8  [[-1, 32, 128, 128], [-1, 32, 256, 256]]               0\n",
      "            Conv2d-9         [-1, 64, 128, 128]          18,496\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-11         [-1, 64, 128, 128]             128\n",
      "           Conv2d-12         [-1, 64, 128, 128]          36,928\n",
      "             ReLU-13         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-14         [-1, 64, 128, 128]             128\n",
      "        MaxPool2d-15           [-1, 64, 64, 64]               0\n",
      "        DownBlock-16  [[-1, 64, 64, 64], [-1, 64, 128, 128]]               0\n",
      "           Conv2d-17          [-1, 128, 64, 64]          73,856\n",
      "             ReLU-18          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
      "           Conv2d-20          [-1, 128, 64, 64]         147,584\n",
      "             ReLU-21          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-22          [-1, 128, 64, 64]             256\n",
      "        MaxPool2d-23          [-1, 128, 32, 32]               0\n",
      "        DownBlock-24  [[-1, 128, 32, 32], [-1, 128, 64, 64]]               0\n",
      "           Conv2d-25          [-1, 256, 32, 32]         295,168\n",
      "             ReLU-26          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-27          [-1, 256, 32, 32]             512\n",
      "           Conv2d-28          [-1, 256, 32, 32]         590,080\n",
      "             ReLU-29          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-30          [-1, 256, 32, 32]             512\n",
      "        DownBlock-31  [[-1, 256, 32, 32], [-1, 256, 32, 32]]               0\n",
      "  ConvTranspose2d-32          [-1, 128, 64, 64]         131,200\n",
      "             ReLU-33          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
      "      Concatenate-35          [-1, 256, 64, 64]               0\n",
      "           Conv2d-36          [-1, 128, 64, 64]         295,040\n",
      "             ReLU-37          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-38          [-1, 128, 64, 64]             256\n",
      "           Conv2d-39          [-1, 128, 64, 64]         147,584\n",
      "             ReLU-40          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-41          [-1, 128, 64, 64]             256\n",
      "          UpBlock-42          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-43         [-1, 64, 128, 128]          32,832\n",
      "             ReLU-44         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-45         [-1, 64, 128, 128]             128\n",
      "      Concatenate-46        [-1, 128, 128, 128]               0\n",
      "           Conv2d-47         [-1, 64, 128, 128]          73,792\n",
      "             ReLU-48         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-49         [-1, 64, 128, 128]             128\n",
      "           Conv2d-50         [-1, 64, 128, 128]          36,928\n",
      "             ReLU-51         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-52         [-1, 64, 128, 128]             128\n",
      "          UpBlock-53         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-54         [-1, 32, 256, 256]           8,224\n",
      "             ReLU-55         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-56         [-1, 32, 256, 256]              64\n",
      "      Concatenate-57         [-1, 64, 256, 256]               0\n",
      "           Conv2d-58         [-1, 32, 256, 256]          18,464\n",
      "             ReLU-59         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-60         [-1, 32, 256, 256]              64\n",
      "           Conv2d-61         [-1, 32, 256, 256]           9,248\n",
      "             ReLU-62         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-63         [-1, 32, 256, 256]              64\n",
      "          UpBlock-64         [-1, 32, 256, 256]               0\n",
      "           Conv2d-65          [-1, 9, 256, 256]             297\n",
      "================================================================\n",
      "Total params: 1,929,129\n",
      "Trainable params: 1,929,129\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 11533808.50\n",
      "Params size (MB): 7.36\n",
      "Estimated Total Size (MB): 11533816.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary = summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 128.0\n",
      "Level 2: 64.0\n",
      "Level 3: 32.0\n",
      "Level 4: 16.0\n",
      "Level 5: 8.0\n",
      "Level 6: 4.0\n",
      "Level 7: 2.0\n",
      "Max-level: 7\n"
     ]
    }
   ],
   "source": [
    "shape = 256\n",
    "def compute_max_depth(shape, max_depth=10, print_out=True):\n",
    "    shapes = []\n",
    "    shapes.append(shape)\n",
    "    for level in range(1, max_depth):\n",
    "        if shape % 2 ** level == 0 and shape / 2 ** level > 1:\n",
    "            shapes.append(shape / 2 ** level)\n",
    "            if print_out:\n",
    "                print(f'Level {level}: {shape / 2 ** level}')\n",
    "        else:\n",
    "            if print_out:\n",
    "                print(f'Max-level: {level - 1}')\n",
    "            break\n",
    "\n",
    "    return shapes\n",
    "\n",
    "\n",
    "out = compute_max_depth(shape, print_out=True, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "\n",
    "        batch_iter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class batches(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array(batch_x),np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
