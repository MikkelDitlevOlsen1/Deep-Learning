{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "import torch \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change current directory up to parent, only run 1 time!\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = [0,1,2,3,5,6,10,11,12,19,20,21,22,24,26,28,29,32,33,35,36,39,40,43,45,46,47,50,51,52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads data from clean_data \n",
    "#path need to be set to the clean_data folder\n",
    "def load_data(test=False,Print=False,path=f'{os.path.abspath(os.curdir)}/data/carseg_data/clean_data',nr_img=1499):\n",
    "    train_data_input=[]\n",
    "    train_data_target=[]\n",
    "    path1 = f'{os.path.abspath(os.curdir)}/data/carseg_data/carseg_raw_data/train/photo'\n",
    "    \n",
    "    if test:\n",
    "        for i in range (len(test_id)):\n",
    "            n = test_id[i]\n",
    "            n1 = f'{n}_a'\n",
    "            test1=np.load(f'{path}/{n1}.npy')\n",
    "            inputs=torch.from_numpy(test1[:3])\n",
    "            target=torch.from_numpy(test1[3])\n",
    "            train_data_input.append(inputs)\n",
    "            train_data_target.append(target)\n",
    "            \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        '''\n",
    "        for l in range(1,835): #DOOR\n",
    "            if l < 10:\n",
    "                n3 = f'DOOR_000{l}'\n",
    "                print(n3)\n",
    "            elif 9 < l < 100:\n",
    "                n3 = f'DOOR_00{l}'\n",
    "            else:\n",
    "                n3 = f'DOOR_0{l}'\n",
    "\n",
    "            test3=np.load(f'{path}/{n3}.npy')\n",
    "            inputs3=torch.from_numpy(test3[:3])\n",
    "            target3=torch.from_numpy(test3[3])\n",
    "            #target= torch.unsqueeze(target,0)\n",
    "            train_data_input.append(inputs3)\n",
    "            train_data_target.append(target3)\n",
    "        \n",
    "        \n",
    "        for m in range(1,2002): #OPEL\n",
    "            if m < 10:\n",
    "                #n3 = f'DOOR_000{m}'\n",
    "                n4 = f'OPEL_000{m}'\n",
    "                #print(n3)\n",
    "            elif 9 < m < 100:\n",
    "                #n3 = f'DOOR_00{m}'\n",
    "                n4 = f'OPEL_00{m}'\n",
    "            elif 99 < m < 1000:\n",
    "                #n3 = f'DOOR_0{m}'\n",
    "                n4 = f'OPEL_0{m}'\n",
    "            else:\n",
    "                n4 = f'OPEL_{m}'\n",
    "            try:\n",
    "\n",
    "                test4=np.load(f'{path}/{n4}.npy')\n",
    "                inputs4=torch.from_numpy(test4[:3])\n",
    "                target4=torch.from_numpy(test4[3])\n",
    "                #target= torch.unsqueeze(target,0)\n",
    "                train_data_input.append(inputs4)\n",
    "                train_data_target.append(target4)\n",
    "            except:\n",
    "                if Print:\n",
    "                    print(f'fil nr {n} mangeler')\n",
    "'''\n",
    "        for n in range(nr_img):\n",
    "            n1=n\n",
    "            \n",
    "            if (n1 not in test_id):\n",
    "                n2 = f'{n}_a'\n",
    "                \n",
    "                if os.path.exists(f'{path1}/{n2}.jpg'):\n",
    "                    test2=np.load(f'{path}/{n2}.npy')\n",
    "                    inputs2=torch.from_numpy(test2[:3])\n",
    "                    target2=torch.from_numpy(test2[3])\n",
    "                    #target= torch.unsqueeze(target,0)\n",
    "                    train_data_input.append(inputs2)\n",
    "                    train_data_target.append(target2)\n",
    "            \n",
    "            #if test:\n",
    "            #    if (n1 in test_id):\n",
    "            #        #n1=f'{n}_a'\n",
    "            \n",
    "            if os.path.exists(f'{path1}/{n1}.jpg'):\n",
    "                test1=np.load(f'{path}/{n1}.npy')\n",
    "                inputs=torch.from_numpy(test1[:3])\n",
    "                target=torch.from_numpy(test1[3])\n",
    "                #target= torch.unsqueeze(target,0)\n",
    "                train_data_input.append(inputs)\n",
    "                train_data_target.append(target)\n",
    "                \n",
    "            \n",
    "            '''\n",
    "            try:\n",
    "                \n",
    "                \n",
    "                test1=np.load(f'{path}/{n1}.npy')\n",
    "                inputs=torch.from_numpy(test1[:3])\n",
    "                target=torch.from_numpy(test1[3])\n",
    "                #target= torch.unsqueeze(target,0)\n",
    "                train_data_input.append(inputs)\n",
    "                train_data_target.append(target)\n",
    "\n",
    "                test2=np.load(f'{path}/{n2}.npy')\n",
    "                inputs2=torch.from_numpy(test2[:3])\n",
    "                target2=torch.from_numpy(test2[3])\n",
    "                #target= torch.unsqueeze(target,0)\n",
    "                train_data_input.append(inputs2)\n",
    "                train_data_target.append(target2)\n",
    "            except:\n",
    "                if Print:\n",
    "                    print(f'fil nr {n} mangeler')\n",
    "            '''\n",
    "        \n",
    "    return [train_data_input,train_data_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test and train data\n",
    "test_data=load_data(test=True)\n",
    "train_data=load_data()\n",
    "\n",
    "#split op into x and y\n",
    "test_x, test_y = test_data[0], test_data[1]\n",
    "train_x, train_y = train_data[0], train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:\n",
      "Number of img 30\n",
      "target sahpe torch.Size([256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n",
      "train_data:\n",
      "Number of img 139\n",
      "target sahpe torch.Size([256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#the form of data is:\n",
    "#data[0] is a list with all rgb img's as tensor\n",
    "#data[1] is a list with all targets as tensor  \n",
    "def Get_stats(data):\n",
    "    print(f'Number of img {len(data[0])}')\n",
    "    print(f'target sahpe {data[1][0].shape}')\n",
    "    print(f'input shape {data[0][0].shape}')\n",
    "    \n",
    "    \n",
    "#print some info about data structure\n",
    "print('test_data:')\n",
    "Get_stats(test_data)\n",
    "print('train_data:')\n",
    "Get_stats(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range (len(train_x)):\n",
    "#    a = torch.permute(train_x[i], (1,2,0))\n",
    "#    plt.imshow(a)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:935: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "transforms.CenterCrop(10),\n",
    "transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),)\n",
    "\n",
    "data_transform = torch.jit.script(transforms)\n",
    "#transforms.ToTensor(),\n",
    "\n",
    "#data_transform = transforms.Compose([\n",
    "#       transforms.RandomSizedCrop(256),\n",
    "#        transforms.RandomHorizontalFlip(),\n",
    "#        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                             std=[0.229, 0.224, 0.225])\n",
    "#    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat data strukter\n",
    "from torch.utils import data\n",
    "class batches(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_set, y_set,transform=None):\n",
    "        self.transform=transform\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.inputs_dtype = torch.float32\n",
    "        self.targets_dtype = torch.long\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx]\n",
    "        batch_y = self.y[idx]\n",
    "        if self.transform:\n",
    "            #batch_x = torch.Tensor.numpy(batch_x)\n",
    "            batch_x = self.transform(batch_x)\n",
    "            #batch_x = torch.from_numpy(batch_x)\n",
    "        return batch_x.type(self.inputs_dtype),batch_y.type(self.inputs_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_target(target):\n",
    "    l=[0]*len(target)\n",
    "    for targets in range(len(target)):\n",
    "        l[targets] = one_hot(target[targets].to(torch.int64), num_classes=9)\n",
    "        l[targets] = torch.permute(l[targets], (2,0,1))\n",
    "    return l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_one=one_hot_target(test_y)\n",
    "train_y_one=one_hot_target(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:\n",
      "Number of img 30\n",
      "target sahpe torch.Size([9, 256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n",
      "train_data:\n",
      "Number of img 139\n",
      "target sahpe torch.Size([9, 256, 256])\n",
      "input shape torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print('test_data:')\n",
    "Get_stats([test_x, test_y_one])\n",
    "print('train_data:')\n",
    "Get_stats([train_x, train_y_one])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(input, num_classes):\n",
    "    \"\"\"Convert class index tensor to one hot encoding tensor.\n",
    "    Args:\n",
    "         input: A tensor of shape [N, 1, *]\n",
    "         num_classes: An int of number of class\n",
    "    Returns:\n",
    "        A tensor of shape [N, num_classes, *]\n",
    "    \"\"\"\n",
    "    shape = np.array(input.shape)\n",
    "    shape[1] = num_classes\n",
    "    shape = tuple(shape)\n",
    "    result = torch.zeros(shape)\n",
    "    result = result.scatter_(1, input.cpu(), 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss of binary class\n",
    "    Args:\n",
    "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
    "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    Raise:\n",
    "        Exception if unexpected reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
    "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
    "        target = target.contiguous().view(target.shape[0], -1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
    "\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return loss\n",
    "        else:\n",
    "            raise Exception('Unexpected reduction {}'.format(self.reduction))\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss, need one hot encode input\n",
    "    Args:\n",
    "        weight: An array of shape [num_classes,]\n",
    "        ignore_index: class index to ignore\n",
    "        predict: A tensor of shape [N, C, *]\n",
    "        target: A tensor of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Return:\n",
    "        same as BinaryDiceLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=None, **kwargs):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape == target.shape, 'predict & target shape do not match'\n",
    "        dice = BinaryDiceLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "\n",
    "        for i in range(target.shape[1]):\n",
    "            if i != self.ignore_index:\n",
    "                dice_loss = dice(predict[:, i], target[:, i])\n",
    "                if self.weight is not None:\n",
    "                    assert self.weight.shape[0] == target.shape[1], \\\n",
    "                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n",
    "                    dice_loss *= self.weights[i]\n",
    "                total_loss += dice_loss\n",
    "\n",
    "        return total_loss/target.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creat valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_x=train_x[:20]\n",
    "ttrain_x=train_x[20:]\n",
    "\n",
    "val_y_one=train_y_one[:20]\n",
    "ttrain_y_one=train_y_one[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creats all the layers we need\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Center-crops the encoder_layer to the size of the decoder_layer,\n",
    "    so that merging (concatenation) between levels/blocks is possible.\n",
    "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n",
    "    \"\"\"\n",
    "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n",
    "        ds = encoder_layer.shape[2:]\n",
    "        es = decoder_layer.shape[2:]\n",
    "        assert ds[0] >= es[0]\n",
    "        assert ds[1] >= es[1]\n",
    "        if encoder_layer.dim() == 4:  # 2D\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\n",
    "                            ]\n",
    "        elif encoder_layer.dim() == 5:  # 3D\n",
    "            assert ds[2] >= es[2]\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\n",
    "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\n",
    "                            ]\n",
    "    return encoder_layer, decoder_layer\n",
    "\n",
    "\n",
    "def conv_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.Conv3d\n",
    "    elif dim == 2:\n",
    "        return nn.Conv2d\n",
    "\n",
    "\n",
    "def get_conv_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 3,\n",
    "                   stride: int = 1,\n",
    "                   padding: int = 1,\n",
    "                   bias: bool = True,\n",
    "                   dim: int = 2):\n",
    "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                           bias=bias)\n",
    "\n",
    "\n",
    "def conv_transpose_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.ConvTranspose3d\n",
    "    elif dim == 2:\n",
    "        return nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def get_up_layer(in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 2,\n",
    "                 dim: int = 3,\n",
    "                 up_mode: str = 'transposed',\n",
    "                 ):\n",
    "    if up_mode == 'transposed':\n",
    "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n",
    "\n",
    "\n",
    "def maxpool_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.MaxPool3d\n",
    "    elif dim == 2:\n",
    "        return nn.MaxPool2d\n",
    "\n",
    "\n",
    "def get_maxpool_layer(kernel_size: int = 2,\n",
    "                      stride: int = 2,\n",
    "                      padding: int = 0,\n",
    "                      dim: int = 2):\n",
    "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "def get_activation(activation: str):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'leaky':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU()\n",
    "\n",
    "\n",
    "def get_normalization(normalization: str,\n",
    "                      num_channels: int,\n",
    "                      dim: int):\n",
    "    if normalization == 'batch':\n",
    "        if dim == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "    elif normalization == 'instance':\n",
    "        if dim == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "    elif 'group' in normalization:\n",
    "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, layer_1, layer_2):\n",
    "        x = torch.cat((layer_1, layer_2), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 MaxPool.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 pooling: bool = True,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: str = 2,\n",
    "                 conv_mode: str = 'same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # pooling layer\n",
    "        if self.pooling:\n",
    "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # activation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "\n",
    "        before_pooling = y  # save the outputs before the pooling operation\n",
    "        if self.pooling:\n",
    "            y = self.pool(y)  # pooling\n",
    "        return y, before_pooling\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: int = 3,\n",
    "                 conv_mode: str = 'same',\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        # upconvolution/upsample layer\n",
    "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "        # conv layers\n",
    "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\n",
    "                                    padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act0 = get_activation(self.activation)\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "        # concatenate layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def forward(self, encoder_layer, decoder_layer):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            encoder_layer: Tensor from the encoder pathway\n",
    "            decoder_layer: Tensor from the decoder pathway (to be up'd)\n",
    "        \"\"\"\n",
    "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n",
    "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n",
    "\n",
    "        if self.up_mode != 'transposed':\n",
    "            # We need to reduce the channel dimension with a conv layer\n",
    "            up_layer = self.conv0(up_layer)  # convolution 0\n",
    "        up_layer = self.act0(up_layer)  # activation 0\n",
    "        if self.normalization:\n",
    "            up_layer = self.norm0(up_layer)  # normalization 0\n",
    "\n",
    "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n",
    "        y = self.conv1(merged_layer)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # acivation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "        return y\n",
    "\n",
    "# creat the model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 out_channels: int = 8,\n",
    "                 n_blocks: int = 4,\n",
    "                 start_filters: int = 32,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = 'batch',\n",
    "                 conv_mode: str = 'same',\n",
    "                 dim: int = 2,\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_blocks = n_blocks\n",
    "        self.start_filters = start_filters\n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.conv_mode = conv_mode\n",
    "        self.dim = dim\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "\n",
    "        # create encoder path\n",
    "        for i in range(self.n_blocks):\n",
    "            num_filters_in = self.in_channels if i == 0 else num_filters_out\n",
    "            num_filters_out = self.start_filters * (2 ** i)\n",
    "            pooling = True if i < self.n_blocks - 1 else False\n",
    "\n",
    "            down_block = DownBlock(in_channels=num_filters_in,\n",
    "                                   out_channels=num_filters_out,\n",
    "                                   pooling=pooling,\n",
    "                                   activation=self.activation,\n",
    "                                   normalization=self.normalization,\n",
    "                                   conv_mode=self.conv_mode,\n",
    "                                   dim=self.dim)\n",
    "\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # create decoder path (requires only n_blocks-1 blocks)\n",
    "        for i in range(n_blocks - 1):\n",
    "            num_filters_in = num_filters_out\n",
    "            num_filters_out = num_filters_in // 2\n",
    "\n",
    "            up_block = UpBlock(in_channels=num_filters_in,\n",
    "                               out_channels=num_filters_out,\n",
    "                               activation=self.activation,\n",
    "                               normalization=self.normalization,\n",
    "                               conv_mode=self.conv_mode,\n",
    "                               dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # final convolution\n",
    "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                         bias=True, dim=self.dim)\n",
    "        \n",
    "        # add the list of modules to current module\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.weight, **kwargs)  # weights\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.bias, **kwargs)  # bias\n",
    "\n",
    "    def initialize_parameters(self,\n",
    "                              method_weights=nn.init.xavier_uniform_,\n",
    "                              method_bias=nn.init.zeros_,\n",
    "                              kwargs_weights={},\n",
    "                              kwargs_bias={}\n",
    "                              ):\n",
    "        for module in self.modules():\n",
    "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\n",
    "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        encoder_output = []\n",
    "\n",
    "        # Encoder pathway\n",
    "        for module in self.down_blocks:\n",
    "            x, before_pooling = module(x)\n",
    "            encoder_output.append(before_pooling)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for i, module in enumerate(self.up_blocks):\n",
    "            before_pool = encoder_output[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "        x = torch.softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\n",
    "        d = {self.__class__.__name__: attributes}\n",
    "        return f'{d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out: torch.Size([4, 9, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#creat a model with 4 bloks\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=9,\n",
    "             n_blocks=4,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "\n",
    "x = torch.randn(size=(4,3, 256, 256), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(f'Out: {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 256]             896\n",
      "              ReLU-2         [-1, 32, 256, 256]               0\n",
      "       BatchNorm2d-3         [-1, 32, 256, 256]              64\n",
      "            Conv2d-4         [-1, 32, 256, 256]           9,248\n",
      "              ReLU-5         [-1, 32, 256, 256]               0\n",
      "       BatchNorm2d-6         [-1, 32, 256, 256]              64\n",
      "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
      "         DownBlock-8  [[-1, 32, 128, 128], [-1, 32, 256, 256]]               0\n",
      "            Conv2d-9         [-1, 64, 128, 128]          18,496\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-11         [-1, 64, 128, 128]             128\n",
      "           Conv2d-12         [-1, 64, 128, 128]          36,928\n",
      "             ReLU-13         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-14         [-1, 64, 128, 128]             128\n",
      "        MaxPool2d-15           [-1, 64, 64, 64]               0\n",
      "        DownBlock-16  [[-1, 64, 64, 64], [-1, 64, 128, 128]]               0\n",
      "           Conv2d-17          [-1, 128, 64, 64]          73,856\n",
      "             ReLU-18          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
      "           Conv2d-20          [-1, 128, 64, 64]         147,584\n",
      "             ReLU-21          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-22          [-1, 128, 64, 64]             256\n",
      "        MaxPool2d-23          [-1, 128, 32, 32]               0\n",
      "        DownBlock-24  [[-1, 128, 32, 32], [-1, 128, 64, 64]]               0\n",
      "           Conv2d-25          [-1, 256, 32, 32]         295,168\n",
      "             ReLU-26          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-27          [-1, 256, 32, 32]             512\n",
      "           Conv2d-28          [-1, 256, 32, 32]         590,080\n",
      "             ReLU-29          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-30          [-1, 256, 32, 32]             512\n",
      "        DownBlock-31  [[-1, 256, 32, 32], [-1, 256, 32, 32]]               0\n",
      "  ConvTranspose2d-32          [-1, 128, 64, 64]         131,200\n",
      "             ReLU-33          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
      "      Concatenate-35          [-1, 256, 64, 64]               0\n",
      "           Conv2d-36          [-1, 128, 64, 64]         295,040\n",
      "             ReLU-37          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-38          [-1, 128, 64, 64]             256\n",
      "           Conv2d-39          [-1, 128, 64, 64]         147,584\n",
      "             ReLU-40          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-41          [-1, 128, 64, 64]             256\n",
      "          UpBlock-42          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-43         [-1, 64, 128, 128]          32,832\n",
      "             ReLU-44         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-45         [-1, 64, 128, 128]             128\n",
      "      Concatenate-46        [-1, 128, 128, 128]               0\n",
      "           Conv2d-47         [-1, 64, 128, 128]          73,792\n",
      "             ReLU-48         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-49         [-1, 64, 128, 128]             128\n",
      "           Conv2d-50         [-1, 64, 128, 128]          36,928\n",
      "             ReLU-51         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-52         [-1, 64, 128, 128]             128\n",
      "          UpBlock-53         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-54         [-1, 32, 256, 256]           8,224\n",
      "             ReLU-55         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-56         [-1, 32, 256, 256]              64\n",
      "      Concatenate-57         [-1, 64, 256, 256]               0\n",
      "           Conv2d-58         [-1, 32, 256, 256]          18,464\n",
      "             ReLU-59         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-60         [-1, 32, 256, 256]              64\n",
      "           Conv2d-61         [-1, 32, 256, 256]           9,248\n",
      "             ReLU-62         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-63         [-1, 32, 256, 256]              64\n",
      "          UpBlock-64         [-1, 32, 256, 256]               0\n",
      "           Conv2d-65          [-1, 9, 256, 256]             297\n",
      "================================================================\n",
      "Total params: 1,929,129\n",
      "Trainable params: 1,929,129\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 11533808.50\n",
      "Params size (MB): 7.36\n",
      "Estimated Total Size (MB): 11533816.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary = summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 128.0\n",
      "Level 2: 64.0\n",
      "Level 3: 32.0\n",
      "Level 4: 16.0\n",
      "Level 5: 8.0\n",
      "Level 6: 4.0\n",
      "Level 7: 2.0\n",
      "Max-level: 7\n"
     ]
    }
   ],
   "source": [
    "#cheak number of \"bloks\" and its resalutien on that blok\n",
    "shape = 256\n",
    "def compute_max_depth(shape, max_depth=10, print_out=True):\n",
    "    shapes = []\n",
    "    shapes.append(shape)\n",
    "    for level in range(1, max_depth):\n",
    "        if shape % 2 ** level == 0 and shape / 2 ** level > 1:\n",
    "            shapes.append(shape / 2 ** level)\n",
    "            if print_out:\n",
    "                print(f'Level {level}: {shape / 2 ** level}')\n",
    "        else:\n",
    "            if print_out:\n",
    "                print(f'Max-level: {level - 1}')\n",
    "            break\n",
    "\n",
    "    return shapes\n",
    "\n",
    "\n",
    "out = compute_max_depth(shape, print_out=True, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "\n",
    "        batch_iter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=batches(ttrain_x,ttrain_y_one, transform=data_transform)\n",
    "\n",
    "valdata=batches(val_x,val_y_one)\n",
    "\n",
    "testdata=batches(test_x,test_y_one)\n",
    "\n",
    "\n",
    "#creat data batches\n",
    "dataloader_training = data.DataLoader(dataset=traindata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "dataloader_val = data.DataLoader(dataset=valdata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "dataloader_train= data.DataLoader(dataset=testdata,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fb6d067d0b4c8aa2e82da0e29485c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Progress'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation'), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train the mo\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "# model\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=9,\n",
    "             n_blocks=7,\n",
    "             start_filters=18,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2).to(device)\n",
    "\n",
    "# criterion\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = DiceLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=dataloader_training,\n",
    "                  validation_DataLoader=dataloader_val,\n",
    "                  lr_scheduler=None,\n",
    "                  epochs=20,\n",
    "                  epoch=0,\n",
    "                  notebook=True)\n",
    "\n",
    "# start training\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9294353127479553,\n",
       " 0.9286787986755372,\n",
       " 0.9265846610069275,\n",
       " 0.9288991689682007,\n",
       " 0.9297357678413392,\n",
       " 0.9276700675487518,\n",
       " 0.9212081551551818,\n",
       " 0.9222377240657806,\n",
       " 0.9226917147636413,\n",
       " 0.9223861396312714,\n",
       " 0.9225928783416748,\n",
       " 0.923224949836731,\n",
       " 0.922990483045578,\n",
       " 0.9233444452285766,\n",
       " 0.9219844043254852,\n",
       " 0.9218275487422943,\n",
       " 0.922081857919693,\n",
       " 0.9219469487667084,\n",
       " 0.9222132086753845,\n",
       " 0.9217946887016296]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data,idx):\n",
    "    t=torch.unsqueeze(data[idx][0],0)\n",
    "    with torch.no_grad():\n",
    "        out = model(t)\n",
    "\n",
    "    plt.imshow(torch.argmax(out,dim=1)[0])\n",
    "    plt.show()\n",
    "    #true\n",
    "    plt.imshow(test_y[idx])\n",
    "    plt.show()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 1.0000e+00],\n",
       "          [9.9997e-01, 9.9998e-01, 9.9998e-01,  ..., 9.9998e-01,\n",
       "           9.9998e-01, 9.9965e-01],\n",
       "          [9.9995e-01, 9.9997e-01, 9.9997e-01,  ..., 9.9997e-01,\n",
       "           9.9998e-01, 9.9958e-01],\n",
       "          ...,\n",
       "          [9.9993e-01, 9.9996e-01, 9.9996e-01,  ..., 9.9932e-01,\n",
       "           9.9511e-01, 9.7464e-01],\n",
       "          [9.9990e-01, 9.9995e-01, 9.9995e-01,  ..., 9.9949e-01,\n",
       "           9.9835e-01, 9.9348e-01],\n",
       "          [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "           1.0000e+00, 1.0000e+00]],\n",
       "\n",
       "         [[7.0304e-10, 1.1017e-11, 1.0812e-11,  ..., 1.0584e-11,\n",
       "           1.0458e-11, 9.9781e-10],\n",
       "          [8.3912e-09, 3.1706e-09, 3.2162e-09,  ..., 3.2551e-09,\n",
       "           3.2789e-09, 4.7688e-08],\n",
       "          [1.0844e-08, 4.5144e-09, 4.7046e-09,  ..., 4.2828e-09,\n",
       "           4.3138e-09, 5.3677e-08],\n",
       "          ...,\n",
       "          [1.5006e-08, 6.1776e-09, 6.0312e-09,  ..., 7.6045e-08,\n",
       "           3.8399e-07, 1.4648e-06],\n",
       "          [2.1817e-08, 8.1469e-09, 9.1318e-09,  ..., 6.1000e-08,\n",
       "           1.6394e-07, 4.8741e-07],\n",
       "          [5.8088e-20, 1.6049e-20, 2.4272e-19,  ..., 5.6235e-17,\n",
       "           7.5451e-17, 3.1402e-20]],\n",
       "\n",
       "         [[4.7452e-08, 3.2092e-09, 3.2031e-09,  ..., 3.1304e-09,\n",
       "           3.0274e-09, 5.7539e-08],\n",
       "          [8.2204e-07, 1.3117e-06, 1.3461e-06,  ..., 1.4019e-06,\n",
       "           1.3968e-06, 3.8975e-06],\n",
       "          [1.0257e-06, 1.8403e-06, 1.9452e-06,  ..., 1.8242e-06,\n",
       "           1.8371e-06, 4.4020e-06],\n",
       "          ...,\n",
       "          [1.3253e-06, 2.3515e-06, 2.3233e-06,  ..., 1.7660e-05,\n",
       "           6.1537e-05, 5.7596e-05],\n",
       "          [1.7996e-06, 2.9182e-06, 3.3234e-06,  ..., 1.3126e-05,\n",
       "           2.5793e-05, 1.9534e-05],\n",
       "          [2.3723e-15, 2.7364e-15, 2.2761e-14,  ..., 1.1635e-12,\n",
       "           1.2941e-12, 1.3825e-15]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.8996e-07, 2.4965e-09, 2.4995e-09,  ..., 2.3906e-09,\n",
       "           2.3648e-09, 1.1841e-06],\n",
       "          [2.8796e-05, 1.0828e-05, 1.1667e-05,  ..., 1.1274e-05,\n",
       "           1.1156e-05, 3.3185e-04],\n",
       "          [3.9873e-05, 1.6751e-05, 1.8668e-05,  ..., 1.6337e-05,\n",
       "           1.5710e-05, 3.9322e-04],\n",
       "          ...,\n",
       "          [6.0978e-05, 2.4643e-05, 2.6236e-05,  ..., 6.0954e-04,\n",
       "           4.6819e-03, 2.5136e-02],\n",
       "          [9.3362e-05, 3.2945e-05, 3.9127e-05,  ..., 4.6345e-04,\n",
       "           1.5521e-03, 6.4385e-03],\n",
       "          [1.3396e-16, 4.7435e-17, 9.2109e-16,  ..., 3.4374e-13,\n",
       "           4.5063e-13, 9.5885e-17]],\n",
       "\n",
       "         [[1.0044e-08, 8.8928e-10, 8.7909e-10,  ..., 8.6796e-10,\n",
       "           8.6013e-10, 1.3051e-08],\n",
       "          [3.0150e-07, 8.3798e-07, 8.5158e-07,  ..., 9.0265e-07,\n",
       "           9.2813e-07, 1.4987e-06],\n",
       "          [3.8607e-07, 1.1820e-06, 1.2332e-06,  ..., 1.1649e-06,\n",
       "           1.2191e-06, 1.6948e-06],\n",
       "          ...,\n",
       "          [4.9357e-07, 1.5353e-06, 1.5030e-06,  ..., 1.0912e-05,\n",
       "           3.6925e-05, 2.0330e-05],\n",
       "          [6.9710e-07, 1.9301e-06, 2.2361e-06,  ..., 7.8741e-06,\n",
       "           1.4731e-05, 6.7241e-06],\n",
       "          [2.0726e-15, 3.7174e-15, 2.9298e-14,  ..., 1.1488e-12,\n",
       "           1.2264e-12, 1.0304e-15]],\n",
       "\n",
       "         [[1.1705e-07, 1.4268e-08, 1.4360e-08,  ..., 1.3974e-08,\n",
       "           1.3640e-08, 1.4680e-07],\n",
       "          [9.9806e-07, 1.0756e-06, 1.1038e-06,  ..., 1.1343e-06,\n",
       "           1.1255e-06, 2.9980e-06],\n",
       "          [1.1723e-06, 1.3606e-06, 1.4269e-06,  ..., 1.3573e-06,\n",
       "           1.3597e-06, 3.2546e-06],\n",
       "          ...,\n",
       "          [1.3919e-06, 1.6143e-06, 1.6066e-06,  ..., 6.7424e-06,\n",
       "           1.6148e-05, 1.9879e-05],\n",
       "          [1.7359e-06, 1.8558e-06, 2.0475e-06,  ..., 5.4850e-06,\n",
       "           8.9434e-06, 9.5617e-06],\n",
       "          [8.0369e-16, 4.1356e-16, 3.1594e-15,  ..., 1.6584e-13,\n",
       "           1.9712e-13, 4.2827e-16]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(testdata,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
